{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file into pandas from the working directory\n",
    "df= pd.read_csv('eclipse.csv', header=None, names=['Bug ID','Status','Severity','Keywords','Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the class distribution\n",
    "df.Severity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "df['severity_num'] = df.Severity.map({'non-severe':0, 'severe':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to define X and y (from the eclipse data) for use with COUNTVECTORIZER\n",
    "X= df.Summary\n",
    "y = df.severity_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train=X_train.dropna(how='any',axis=0) \n",
    "y_train = y_train.dropna(how='any',axis=0) \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print message text for the false positives (non-severe incorrectly classified as severe)\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over and Under sampling code starts from here:\n",
    "\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier to use and splitting data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "classifier = RandomForestClassifier #classifier to use\n",
    "data = pd.read_csv(\"eclipse.csv\", header=None, names=['Bug ID','Status','Severity','Keywords','Summary']) #data reading\n",
    "data.head(5)\n",
    "data['severity_num'] = data.Severity.map({'non-severe':0, 'severe':1})\n",
    "data_feature = data.Summary\n",
    "data_target = data.severity_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_feature, data_target, random_state=42)\n",
    "X_train=X_train.dropna(how='any',axis=0) \n",
    "y_train = y_train.dropna(how='any',axis=0) #null value dropping from anywhere\n",
    "vect = CountVectorizer() #vectorizing\n",
    "vect.fit(X_train) #train data fitting \n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building normal model using transformed data\n",
    "import numpy as np\n",
    "pipeline = make_pipeline(classifier(random_state=42, ))\n",
    "X_test = np.random.rand(len(X_train), 1)\n",
    "model = pipeline.fit(X_train_dtm, y_train)\n",
    "prediction = model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building and fitting data inside the model with Oversampling (SMOTE) imblearn\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=42), classifier(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train_dtm, y_train)\n",
    "smote_prediction = smote_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building and fitting data inside the model with undersampling (NearMiss)\n",
    "from imblearn.under_sampling import NearMiss\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train_dtm, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print information about both models\n",
    "print()\n",
    "data = pd.read_csv(\"eclipse.csv\", header=None, names=['Bug ID','Status','Severity','Keywords','Summary'])\n",
    "data['severity_num'] = data.Severity.map({'non-severe':0, 'severe':1})\n",
    "data_feature = data.Summary\n",
    "data_target = data.severity_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_feature, data_target, random_state=42)\n",
    "X_train=X_train.dropna(how='any',axis=0) \n",
    "y_train = y_train.dropna(how='any',axis=0) \n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "print(\"normal data distribution: {}\".format(Counter(data_target)))\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(X_train_dtm, y_train)\n",
    "print(\"Smote data distribution: {}\".format(Counter(y_smote)))\n",
    "\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(X_train_dtm, y_train)\n",
    "print(\"Nearmiss data distribution: {}\".format(Counter(y_nearmiss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, prediction))\n",
    "print(classification_report_imbalanced(y_test, smote_prediction))\n",
    "\n",
    "print()\n",
    "print('Normal Pipeline Score {}'.format(pipeline.score(X_test_dtm, y_test)))\n",
    "print('SMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test_dtm, y_test)))\n",
    "print('NearMiss Pipeline Score {}'.format(nearmiss_pipeline.score(X_test_dtm, y_test)))\n",
    "\n",
    "print()\n",
    "print_results(\"Normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal kfold was unable to read the test data properly.So we use Stratified kfold because, .\n",
    "#Stratified K-Folds cross-validator provides train/test indices to split data in train/test sets.\n",
    "#This cross-validation object is a variation of KFold that returns stratified folds.\n",
    "#The folds are made by preserving the percentage of samples for each class.\n",
    "#https://www.youtube.com/watch?v=p7ij9sCYEbA\n",
    "# CrossValidation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2)\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "auc =[]\n",
    "data = pd.read_csv(\"eclipse.csv\", header=None, names=['Bug ID','Status','Severity','Keywords','Summary'])\n",
    "data.head(5)\n",
    "data['severity_num'] = data.Severity.map({'non-severe':0, 'severe':1})\n",
    "data_feature = data.Summary\n",
    "data_target = data.severity_num\n",
    "data_feature=data_feature.dropna(how='any',axis=0) \n",
    "data_target = data_target.dropna(how='any',axis=0) \n",
    "vect = CountVectorizer()\n",
    "vect.fit(data_feature)\n",
    "for train, test in skf.split(data_feature, data_target):\n",
    "    \n",
    "    X_train_dtm = vect.transform(data_feature[train])\n",
    "    X_test_dtm = vect.transform(data_feature[test])\n",
    "    pipeline = make_pipeline_imb(SMOTE(), classifier(random_state=2))\n",
    "    model = pipeline.fit(X_train_dtm, data_target[train])\n",
    "    prediction = model.predict(X_test_dtm)\n",
    "    accuracy.append(pipeline.score(X_test_dtm, data_target[test]))\n",
    "    precision.append(precision_score(data_target[test], prediction))\n",
    "    recall.append(recall_score(data_target[test], prediction))\n",
    "    f1.append(f1_score(data_target[test], prediction))\n",
    "    auc.append(roc_auc_score(data_target[test], prediction))\n",
    "    \n",
    "\n",
    "print()\n",
    "print(\"mean of scores 5-fold:\" )\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy)))\n",
    "print(\"precision: {}\".format(np.mean(precision)))\n",
    "print(\"recall: {}\".format(np.mean(recall)))\n",
    "print(\"f1: {}\".format(np.mean(f1)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
